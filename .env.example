# ── Model configuration ─────────────────────────────────────
# Generation model used for HyDE + final answer synthesis
GENERATION_MODEL=qwen3:4b-q4_K_M

# Embedding model (must be available in your Ollama instance)
EMBEDDING_MODEL=qwen3-embedding:0.6b

# ── ChromaDB ─────────────────────────────────────────────────
CHROMADB_PORT=8088
CHROMA_COLLECTION=handbooks

# ── Ollama ───────────────────────────────────────────────────
OLLAMA_PORT=11434

# ── Orchestration API ────────────────────────────────────────
ORCHESTRATION_PORT=8089

# Number of document chunks to retrieve per query
HYDE_NUM_RESULTS=5

# Include per-chapter summaries in retrieval context (true/false)
# When true, the LLM receives broader chapter context alongside specific chunks.
# Set to false if large documents cause context bloat.
INCLUDE_CHAPTER_SUMMARIES=true

# ── Ingestion UI ─────────────────────────────────────────────
INGESTION_UI_PORT=8501
